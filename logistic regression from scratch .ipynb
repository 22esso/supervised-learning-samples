{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcuNFyEACvYs"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from keras.datasets import mnist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRN1_RRaCvYu"
      },
      "outputs": [],
      "source": [
        "def standardize(data, ax):\n",
        "    mean = np.sum(data, axis=ax) / len(data)\n",
        "    variance = np.sum((data - mean) ** 2) / len(data)\n",
        "    std_deviation = np.sqrt(variance)\n",
        "\n",
        "    standardized_data = (data - mean) / std_deviation\n",
        "    return standardized_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-S9H0K9CvYu"
      },
      "outputs": [],
      "source": [
        "def K_fold(data, K=5):\n",
        "    fold_size = data.shape[0] // K\n",
        "    folds = np.zeros([K, fold_size, 785])\n",
        "    ready_data = np.array(data, dtype=object)\n",
        "    for i in range(K):\n",
        "        indices = np.random.choice(ready_data.shape[0], size=fold_size, replace=False)\n",
        "        work_fold = ready_data[indices]\n",
        "        folds[i] = np.array(work_fold)\n",
        "        ready_data = np.delete(ready_data, indices, axis=0)\n",
        "    folds = np.array(folds)\n",
        "    return folds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QM-0GpICvYv"
      },
      "outputs": [],
      "source": [
        "def rand_weights(size):\n",
        "    dum = list()\n",
        "    for i in range(size):\n",
        "        dum.append(random.uniform(-1, 1))\n",
        "    return np.array(dum)\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    X = [(1 / (1 + np.exp(-z))) for z in x]\n",
        "    return np.array(X)\n",
        "\n",
        "\n",
        "def error(H, Y):\n",
        "    er = 0\n",
        "    for i in range(len(H)):\n",
        "        er = er + ((Y[i] * np.log(H[i])) + ((1 - Y[i]) * np.log(1 - H[i])))\n",
        "    return round(er / len((H)), 4)\n",
        "\n",
        "\n",
        "def accuracy(class_f, Y_test):\n",
        "    return (np.sum(class_f == Y_test) / len(Y_test)) * 100\n",
        "\n",
        "\n",
        "class LogisticRegression():\n",
        "    def __init__(self, learning_rate=0.05, maxIter=1000, error_ratio=0.01):\n",
        "        self.__learning_rate = learning_rate\n",
        "        self.__maxIter = maxIter\n",
        "        self.__weigths = None\n",
        "        self.__bias = 0\n",
        "        self.__error_ratio = error_ratio\n",
        "\n",
        "    def fit(self, X, Y):\n",
        "        sample_size = np.array(X).shape[0]\n",
        "        n_features = np.array(X).shape[1]\n",
        "        #self.__weigths = np.zeros(n_features)\n",
        "        self.__weigths = rand_weights(n_features)\n",
        "        Error = 1\n",
        "        epoch_bar = tqdm(desc='Epochs', total=self.__maxIter)\n",
        "        for i in range(self.__maxIter):\n",
        "            epoch_bar.update(1)\n",
        "            epoch_bar.set_postfix({'accuracy': f'{1 - Error:.3f}'})\n",
        "            linear = np.dot(X, self.__weigths) + self.__bias\n",
        "            prediction = sigmoid(linear)\n",
        "            dw = (1 / sample_size) * np.dot(X.T, (prediction - Y))\n",
        "            db = (1 / sample_size) * np.sum(prediction - Y)\n",
        "\n",
        "            self.__weigths = self.__weigths - self.__learning_rate * dw\n",
        "            self.__bias = self.__bias - self.__learning_rate * db\n",
        "            Error = abs(error(prediction, Y))\n",
        "            if self.__error_ratio > Error:\n",
        "                print(Error)\n",
        "                break\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        linear = np.dot(X_test, self.__weigths) + self.__bias\n",
        "        Y_predicted = sigmoid(linear)\n",
        "        class_f = [1 if y > 0.5 else 0 for y in Y_predicted]\n",
        "        return class_f\n",
        "\n",
        "    def get_weights(self):\n",
        "        return self.__weigths\n",
        "\n",
        "    def set_weights(self, weights):\n",
        "        self.__weigths = weights\n",
        "\n",
        "    def get_learning_rate(self):\n",
        "        return self.__learning_rate\n",
        "\n",
        "    def set_learning_rate(self, learning_rate):\n",
        "        self.__learning_rate = learning_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aF1sj-gsCvYw"
      },
      "outputs": [],
      "source": [
        "def model(x_train, y_train, x_test, y_test, l_r):\n",
        "    model = LogisticRegression(learning_rate=l_r, maxIter=1000)\n",
        "    model.fit(x_train, y_train)\n",
        "    pred = model.predict(x_test)\n",
        "    acc = accuracy(pred, y_test)\n",
        "    return acc, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyv031XFCvYw",
        "outputId": "a58e5650-e067-47ef-9b7c-c15646e095c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "(train_X, train_y), (test_X, test_y) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Reo0VRkCvYw",
        "outputId": "a6b2c8bf-4691-4e7b-8295-55801bb581c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(60000, 28, 28) (60000,) (10000, 28, 28) (10000,)\n"
          ]
        }
      ],
      "source": [
        "train_X = np.array(train_X)\n",
        "train_y = np.array(train_y)\n",
        "test_X = np.array(test_X)\n",
        "test_y = np.array(test_y)\n",
        "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UI1IzoAeCvYx",
        "outputId": "18a19ad8-e097-4882-c0a3-98ffe63fe539"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(12665, 28, 28) (12665, 1) (2115, 28, 28) (2115, 1)\n"
          ]
        }
      ],
      "source": [
        "train_X = np.append(train_X[np.where(train_y == 0)], train_X[np.where(train_y == 1)], axis=0)\n",
        "train_y = np.append(train_y[np.where(train_y == 0)], train_y[np.where(train_y == 1)], axis=0).reshape(-1, 1)\n",
        "test_X = np.append(test_X[np.where(test_y == 0)], test_X[np.where(test_y == 1)], axis=0)\n",
        "test_y = np.append(test_y[np.where(test_y == 0)], test_y[np.where(test_y == 1)], axis=0).reshape(-1, 1)\n",
        "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hN-J87nCvYy",
        "outputId": "c91b191b-02ec-4cdc-fb9e-ca1390656052"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(12665, 784) (2115, 784)\n"
          ]
        }
      ],
      "source": [
        "x_train = train_X.reshape(-1, 28 * 28)\n",
        "x_train = np.array(standardize(x_train, 0))\n",
        "x_test = test_X.reshape(-1, 28 * 28)\n",
        "x_test = np.array(standardize(x_test, 0))\n",
        "print(x_train.shape, x_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4LfkLrGCvYy",
        "outputId": "361abcce-85dc-4099-947c-6a38b2d3c070"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(12665, 785)\n"
          ]
        }
      ],
      "source": [
        "train_data = np.append(x_train, train_y, axis=1)\n",
        "print(train_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPNWMMXcCvYz"
      },
      "outputs": [],
      "source": [
        "learning_rates = [0.1, 0.01, 0.001, 0.0001]\n",
        "validation_accuarcy = []\n",
        "test_accuarcy = []\n",
        "models = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EqdAlYTCvYz",
        "outputId": "9f4353c0-6d69-441c-ea9a-447d583dd66e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs: 100%|██████████| 1000/1000 [02:52<00:00,  5.79it/s, accuracy=0.936]\n",
            "Epochs: 100%|██████████| 1000/1000 [02:52<00:00,  5.81it/s, accuracy=0.935]\n",
            "Epochs: 100%|██████████| 1000/1000 [02:52<00:00,  5.81it/s, accuracy=0.936]\n",
            "Epochs: 100%|██████████| 1000/1000 [02:53<00:00,  5.77it/s, accuracy=0.935]\n",
            "Epochs: 100%|██████████| 1000/1000 [02:52<00:00,  5.78it/s, accuracy=0.934]\n",
            "Epochs: 100%|██████████| 1000/1000 [02:51<00:00,  5.83it/s, accuracy=0.937]\n",
            "Epochs: 100%|██████████| 1000/1000 [02:52<00:00,  5.79it/s, accuracy=0.933]\n",
            "Epochs: 100%|██████████| 1000/1000 [02:43<00:00,  6.13it/s, accuracy=0.936]\n",
            "Epochs: 100%|██████████| 1000/1000 [02:41<00:00,  6.19it/s, accuracy=0.930]\n",
            "Epochs: 100%|██████████| 1000/1000 [02:40<00:00,  6.22it/s, accuracy=0.933]\n",
            "Epochs: 100%|██████████| 1000/1000 [02:41<00:00,  6.20it/s, accuracy=0.629]\n",
            "Epochs: 100%|██████████| 1000/1000 [02:40<00:00,  6.23it/s, accuracy=0.605]\n",
            "Epochs: 100%|██████████| 1000/1000 [02:47<00:00,  5.98it/s, accuracy=0.623]\n",
            "Epochs: 100%|██████████| 1000/1000 [02:50<00:00,  5.85it/s, accuracy=0.703]\n",
            "Epochs: 100%|██████████| 1000/1000 [02:48<00:00,  5.92it/s, accuracy=0.625]\n",
            "Epochs: 100%|██████████| 1000/1000 [02:48<00:00,  5.92it/s, accuracy=0.690]\n",
            "Epochs: 100%|██████████| 1000/1000 [02:48<00:00,  5.92it/s, accuracy=0.702]\n",
            "Epochs: 100%|██████████| 1000/1000 [02:48<00:00,  5.93it/s, accuracy=0.645]\n",
            "Epochs: 100%|██████████| 1000/1000 [02:47<00:00,  5.98it/s, accuracy=0.701]\n",
            "Epochs: 100%|██████████| 1000/1000 [02:46<00:00,  6.01it/s, accuracy=0.694]\n",
            "Epochs: 100%|██████████| 1000/1000 [02:48<00:00,  5.94it/s, accuracy=0.253]\n",
            "Epochs: 100%|██████████| 1000/1000 [02:47<00:00,  5.98it/s, accuracy=0.301]\n",
            "Epochs: 100%|██████████| 1000/1000 [02:46<00:00,  6.00it/s, accuracy=0.340]\n",
            "Epochs: 100%|██████████| 1000/1000 [02:48<00:00,  5.95it/s, accuracy=0.463]\n",
            "Epochs: 100%|██████████| 1000/1000 [02:49<00:00,  5.90it/s, accuracy=0.437]\n",
            "Epochs: 100%|██████████| 1000/1000 [02:46<00:00,  5.99it/s, accuracy=0.423]\n",
            "Epochs: 100%|██████████| 1000/1000 [02:44<00:00,  6.08it/s, accuracy=0.194]\n",
            "Epochs: 100%|██████████| 1000/1000 [02:41<00:00,  6.20it/s, accuracy=0.418]\n",
            "Epochs: 100%|██████████| 1000/1000 [02:40<00:00,  6.22it/s, accuracy=0.450]\n",
            "Epochs: 100%|██████████| 1000/1000 [02:40<00:00,  6.22it/s, accuracy=0.293]\n",
            "Epochs: 100%|██████████| 1000/1000 [02:40<00:00,  6.24it/s, accuracy=0.473]\n",
            "Epochs: 100%|██████████| 1000/1000 [02:40<00:00,  6.24it/s, accuracy=0.420]\n",
            "Epochs: 100%|██████████| 1000/1000 [02:40<00:00,  6.23it/s, accuracy=0.280]\n",
            "Epochs: 100%|██████████| 1000/1000 [02:41<00:00,  6.18it/s, accuracy=0.444]\n",
            "Epochs: 100%|██████████| 1000/1000 [02:39<00:00,  6.28it/s, accuracy=0.280]\n",
            "Epochs: 100%|██████████| 1000/1000 [02:40<00:00,  6.24it/s, accuracy=0.212]\n",
            "Epochs: 100%|██████████| 1000/1000 [02:40<00:00,  6.21it/s, accuracy=0.194]\n",
            "Epochs: 100%|██████████| 1000/1000 [02:41<00:00,  6.20it/s, accuracy=0.361]\n",
            "Epochs: 100%|██████████| 1000/1000 [02:40<00:00,  6.24it/s, accuracy=0.391]\n",
            "Epochs: 100%|██████████| 1000/1000 [02:40<00:00,  6.25it/s, accuracy=0.439]\n"
          ]
        }
      ],
      "source": [
        "for i in learning_rates:\n",
        "    training = K_fold(train_data, 10)\n",
        "    for j in range(10):\n",
        "        val = training[j]\n",
        "        x_val = np.array(val[:, 0:784])\n",
        "        y_val = np.array(val[:, 784])\n",
        "\n",
        "        train = np.delete(training, j, axis=0).reshape(-1, 785)\n",
        "        x_train = np.array(train[:, 0:784])\n",
        "        y_train = np.array(train[:, 784])\n",
        "\n",
        "        Acc, Model = model(x_train, y_train, x_val, y_val, i)\n",
        "\n",
        "        validation_accuarcy.append(Acc)\n",
        "        models.append(Model.get_weights())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fu65LgRsCvY0",
        "outputId": "d27bc71b-8e72-4b87-83f5-1bbc4f1d2426"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "99.76303317535546 2 0.1\n"
          ]
        }
      ],
      "source": [
        "Max = max(validation_accuarcy)\n",
        "idx = validation_accuarcy.index(Max)\n",
        "weights = models[idx]\n",
        "learning_rate = learning_rates[(idx // 10)]\n",
        "print(Max, idx, learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "aHWedERTCvY0",
        "outputId": "6fc207d5-6985-46f1-a53a-07afd79aa721"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "acc on test = 106376.59574468085\n"
          ]
        }
      ],
      "source": [
        "cls = LogisticRegression(learning_rate=learning_rate)\n",
        "cls.set_weights(weights)\n",
        "predictions = cls.predict(x_test)\n",
        "print(f\"acc on test = {accuracy(predictions, test_y)}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
